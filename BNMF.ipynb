{"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"DM9cd2i-ufBX"},"source":["# Filtrado Colaborativo: *Bayesian Non-negative Matrix Factorization*\n","\n","El modelo NMF supone un gran avance para la justificación de las predicciones proporcionas por el sistema a la vez que mantiene los buenos resultados presentados por el modelo PMF. Sin embargo, a pesar de eliminar los valores negativos en los factores, las predicciones siguen siendo difilies de explicar.\n","\n","¿En qué rango se mueven los factores? ¿Que interpretación tiene cada valor numérico? ¿Qué un factor valga 0.8 quiere decir que es el doble de importante que uno que valga 0.4? Todas estas preguntas plantean dudas al usuario a la hora de entender qué esta haciendo el modelo y, por tanto, de confiar en los resultados que proporciona.\n","\n","La explicación de recomendaciones en modelos de factorización matricial pasa por dar un significado probabilístico a los valores de los factores latentes. Si se logra que todos los factores tomen valores en el rango \\[0, 1], será posible justificar las recomendaciones. De este modo, si un usuario tiene el valor 0.5 en el factor 2, querrá decir que existe una probabilidad de 0.5 de que al usuario le guste un item relacionado con el factor 2.\n","\n","El modelo *BNMF (**Bayesian Non-negative Matrix Factorization**)* se basa en el principio de la factorización matricial, pero aporta un significado probabilístico a los factores. A pesar de estar basado en la misma idea, el modelo de BNMF difiere bastante de PMF y NMF. BNMF se basa en el modelo bayesiano que puede observarse en la siguiente imagen:\n","\n","\n","<img src=\"https://drive.google.com/uc?export=view&id=1ATFFIKDGGL1TJntfjiRoFrh7Mkwrl3pE\" alt=\"Datos empleados por los sistemas de recomendación\">\n","\n","Donde:\n","\n","- $\\alpha$ representa un parámetro del modelo que simboliza la probabilidad de obtener solapamiento entre los diferentes *clusters* que forma el subespacio de factores latentes.\n","\n","- $\\beta$ representa un parámetro del modelo relacionado con el número de evidencias necesarias para formar parte de un *cluster*.\n","\n","- $\\kappa_{i,k}$ es una variable aleatoria de una distribución Beta. Esta variable representa la probabilidad de que a un usuario del grupo $k$ le guste el item $i$.\n","\n","- $\\vec{\\theta}_u$ es un vector *k* dimensional de la variable aleatoria de una distribución Dirichlet. Este vector simboliza la probabilidad de que un usuario pertenezca a un grupo. Se emplea la distribución de Dirichlet porque conjuga perfectamente con la distribución de la variable categórica.\n","\n","- $z_{u,i}$ es una variable aleatoria de la distribución categórica. Representa que el usuario $u$ votará el item $i$ si el usuario pertenece al grupo *k*.\n","\n","- $\\rho_{u,i}$ es una variable aleatoria de la distribución Binomial. La distribución Binomial permite modelar los votos discretos existente en la mayoría de los sistemas de recomendación.\n","\n","Más allá de las matemáticas, debemos comprender que este modelo representa mejor un sistema de recomendación que otras implementaciones de la factorización matricial. La mayoría de modelos asumen una distribución normal de las votaciones al optimizar una función en base al error cuadrático medio. Por contra, BNMF asume que los votos de los usuarios son discretos y finitos, tal y como sucede cuando realizamos votaciones en una escala de 1 a 5 estrellas.\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"WlQR5g2CntZT"},"source":["## Entrenamiento del modelo\n","\n","Debido a las características bayesianas del modelo, el proceso de entrenamiento no puede realizarse mediante la técnica del descenso del gradiente. En su lugar se emplea inferencia variacional para obtener el conjunto de ecuaciones necesarias para ajustar los valores de los factores a los datos de entrenamiento.\n","\n","Los factores de los usuarios se representan mediante el carácter $a$ y se calculan de acuerdo con la siguiente ecuación:\n","\n","$$a_{u,k}=\\frac {\\gamma_{u,k}} {\\sum_{f=1..K} \\gamma_{u,f}}$$\n","\n","Donde $\\gamma_{u,k}$ se calcula:\n","\n","$$\\gamma_{u,k} = \\alpha + \\sum_{\\{i | r_{u,i} \\neq \\bullet \\}} \\lambda_{u,i,k}$$\n","\n","Los factores de los usuarios se representan mediante el carácter $b$ y se calculan de acuerdo con la siguiente ecuación:\n","\n","$$b_{k,i} = \\frac{\\epsilon_{i,k}^+}{\\epsilon_{i,k}^+ + \\epsilon_{i,k}^-}$$\n","\n","Donde $\\epsilon_{i,k}^+$ y $\\epsilon_{i,k}^-$ se calculan:\n","\n","$$\\epsilon_{i,k}^+ = \\beta + \\sum_{\\{i | r_{u,i} \\neq \\bullet \\}} \\lambda_{u,i,k} \\cdot r_{u,i}^+$$\n","\n","$$\\epsilon_{i,k}^- = \\beta + \\sum_{\\{i | r_{u,i} \\neq \\bullet \\}} \\lambda_{u,i,k} \\cdot r_{u,i}^-$$\n","\n","Verificándose que:\n","\n","$$r_{u,i}^+ = R \\cdot r_{u,i}^*$$\n","\n","$$r_{u,i}^- = R \\cdot (1 - r_{u,i}^*)$$\n","\n","Donde $r_{u,i}^*$ es el voto del usuario $u$ al item $i$ normalizado.\n","\n","Todas las anteriores ecuaciones dependen del valor de $\\lambda_{u,i,k}$, el cual puede calcularse como:\n","\n","$$\\lambda_{u,i,k} = \\frac {\\lambda_{u,i,k}^\\prime} {\\lambda_{u,i,1}^\\prime + \\cdots + \\lambda_{u,i,K}^\\prime}$$\n","\n","$$\\lambda_{u,i,k}^\\prime = exp( \\Psi(\\gamma_{u,k}) + r_{u,i}^+ \\cdot \\Psi(\\epsilon_{i,k}^+) +  r_{u,i}^- \\cdot \\Psi(\\epsilon_{i,k}^-) - R \\cdot \\Psi(\\epsilon_{i,k}^+ + \\epsilon_{i,k}^-))$$\n","\n","Siendo $\\Psi la función diggama definida como la derivada logarítmica de la función gamma.\n","\n","Dadas estas ecuaciones, el algoritmo de entrenamiento quedaría del siguiente modo:\n","\n","\n","```\n","Iniciar aleatoriamente los valores de $\\gamma_{u,k}$\n","Iniciar aleatoriamente los valores de $\\epsilon_{i,k}^+$\n","Iniciar aleatoriamente los valores de $\\epsilon_{i,k}^-$\n","\n","Iterar hasta la convergencia:\n","\n","    Para cada usuario $u$:\n","        Para cada item $i$ votado por el usuario $u$:\n","            Para cada factor $k$:\n","                Actualizar $\\lambda_{u,k,i}$\n","                \n","    Para cada usuario $u$:\n","        Para cada factor $k$:\n","            Actualizar $\\gamma_{u,k}$\n","            \n","        Para cada item $i$ votado por el usuario $u$:\n","            Para cada factor $k$:\n","                Actualizar $\\epsilon_{i,k}^+$\n","                Actualizar $\\epsilon_{i,k}^-$\n","\n","Calcular $a_{u,k}$\n","Calcular $b_{i,k}$\n","```\n","\n","Una vez entrenando el modelo la predicción del voto del usuario $u$ al item $i$ puede calcularse como:\n","\n","$$\\hat{r}_{u,i} = \\sum_{k=1..K} a_{u,k} \\cdot b_{i,k}$$\n","\n","Téngase en cuenta que esta predicción estará normalizada.\n","\n","Veamos cómo hacer esto con código.\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"YZSFkDqsuLes"},"source":["## Carga del dataset\n","\n","Para ilustrar mejor el funcionamiento el algoritmo BNMF, vamos a desarrollar una implementación del mismo.\n","\n","Para ello usaremos el dataset de [MovieLens 100K](https://grouplens.org/datasets/movielens/) que contiene 100.000 votos de 943 usuarios sobre 1682 películas. Este dataset ha sido dividido en votaciones de entrenamiento (80%) y votaciones de test (20%). Además, los códigos de usuarios e items, han sido modificados para que comience en 0 y terminen en el número de (usuarios / items) - 1.\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"9yhGC0WIuORP"},"source":["Inicialmente definimos algunas constantes que nos serán necesarias durante la codificación del algoritmo:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{},"colab_type":"code","id":"7pXAbayjuSqc"},"outputs":[],"source":["import random\n","from scipy.special import digamma\n","from math import exp"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{},"colab_type":"code","id":"tNuX_hVEuQe5"},"outputs":[],"source":["NUM_USERS = 943\n","NUM_ITEMS = 1682\n","\n","MIN_RATING = 1\n","MAX_RATING = 5"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ycfkDZZDuVkq"},"source":["Y cargamos el dataset:"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{},"colab_type":"code","id":"C7S5kGa1uSKU"},"outputs":[],"source":["ratings = [[None for _ in range(NUM_ITEMS)] for _ in range(NUM_USERS)] \n","\n","with open(\"./ml-100k/u1.base\", \"rb\") as training_file:\n","  for line in training_file:\n","    [u, i, rating] = line.decode(\"utf-8\").split(\"\\t\")[:3]\n","    ratings[int(u)-1][int(i)-1] = int(rating)"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Qr4uRdaqua7Q"},"source":["Del mismo modo, cargamos la matriz de votaciones de test:"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{},"colab_type":"code","id":"6e5qWLRRucaO"},"outputs":[],"source":["test_ratings = [[None for _ in range(NUM_ITEMS)] for _ in range(NUM_USERS)] \n","\n","with open(\"./ml-100k/u1.test\", \"rb\") as test_file:\n","  for line in test_file:\n","    [u, i, rating] = line.decode(\"utf-8\").split(\"\\t\")[:3]\n","    test_ratings[int(u)-1][int(i)-1] = int(rating)"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"YpG1RK15uld0"},"source":["## Entrenamiento del modelo\n","\n","Veamos cómo podemos aplicar las fórmulas anteriores para entrenar el modelo.\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"iSb2-aXCurQy"},"source":["Primero, definimos los parámetros del modelo:"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{},"colab_type":"code","id":"T-V28H_juQ-X"},"outputs":[],"source":["NUM_FACTORS = 5\n","ALPHA = 0.8\n","BETA = 5\n","R = 4"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"5FV38gxWuQ-Z"},"source":["Inicializamos las matrices $\\gamma$, $\\epsilon^+$ y $\\epsilon^-$ necesarias para el cálculo de los factores con valores uniformes aleatorios en el intervalo \\[0, 1)."]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{},"colab_type":"code","id":"7wCL1avRuQ-a"},"outputs":[],"source":["gamma = [[random.random() for _ in range(NUM_FACTORS)] for _ in range(NUM_USERS)] \n","ep = [[random.random() for _ in range(NUM_FACTORS)] for _ in range(NUM_ITEMS)] \n","em = [[random.random() for _ in range(NUM_FACTORS)] for _ in range(NUM_ITEMS)] "]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"EWJUu6UAuQ-b"},"source":["Vamos a hacer que el modelo aprenda. Ejecutamos tantas veces como iteraciones haya la actualización de los factores. Para ello, recorremos el conjunto de votos y vamos haciendo las actualizaciones correspondientes."]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{},"colab_type":"code","id":"1Q21B9BQWIGS"},"outputs":[],"source":["NUM_ITERATIONS = 5"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":107},"colab_type":"code","executionInfo":{"elapsed":35227,"status":"ok","timestamp":1558733447067,"user":{"displayName":"Fernando Ortega Requena","photoUrl":"https://lh5.googleusercontent.com/-t9XtZoyOrPU/AAAAAAAAAAI/AAAAAAAAA2I/muQCKCLOQqk/s64/photo.jpg","userId":"02003917424124170753"},"user_tz":-120},"id":"wqfRa1ThuQ-c","outputId":"12b32b48-d140-4b54-95ea-f878a9d2a515"},"outputs":[{"name":"stdout","output_type":"stream","text":["Iteración 1 de 5\n","Iteración 2 de 5\n","Iteración 3 de 5\n","Iteración 4 de 5\n","Iteración 5 de 5\n"]}],"source":["for it in range(NUM_ITERATIONS):\n","  print(\"Iteración \" + str(it + 1) + \" de \" + str(NUM_ITERATIONS))\n","  \n","  # Calculamos lambda\n","  \n","  lmbda = [[[0 for _ in range(NUM_FACTORS)] for _ in range(NUM_ITEMS)] for _ in range(NUM_USERS)]\n","    \n","  for u in range(NUM_USERS):\n","    for i in range(NUM_ITEMS):\n","      if ratings[u][i] != None:\n","        \n","        r = (ratings[u][i] - MIN_RATING) / (MAX_RATING - MIN_RATING)\n","        \n","        rp = R * r\n","        rm = R * (1 - r)\n","               \n","        for k in range(NUM_FACTORS):                        \n","          lmbda[u][i][k] = exp(\n","            digamma(gamma[u][k]) +\n","            rp * digamma(ep[i][k]) + \n","            rm * digamma(em[i][k]) -\n","            R * digamma(ep[i][k] + em[i][k])\n","          )\n","                                        \n","        for k in range(NUM_FACTORS):\n","          lmbda[u][i][k] = lmbda[u][i][k] / sum(lmbda[u][i])\n","  \n","  # Reinicioamos gamma, epsilon+ y epsilon-\n","  \n","  gamma = [[ALPHA for _ in range(NUM_FACTORS)] for _ in range(NUM_USERS)] \n","  ep = [[BETA for _ in range(NUM_FACTORS)] for _ in range(NUM_ITEMS)] \n","  em = [[BETA for _ in range(NUM_FACTORS)] for _ in range(NUM_ITEMS)]  \n","  \n","  \n","  # Actualizamos\n","  \n","  for u in range(NUM_USERS):\n","    for i in range(NUM_ITEMS):\n","      if ratings[u][i] != None:\n","        \n","        r = (ratings[u][i] - MIN_RATING) / (MAX_RATING - MIN_RATING)\n","        \n","        rp = R * r\n","        rm = R * (1 - r)\n","        \n","        for k in range(NUM_FACTORS):\n","          gamma[u][k] += lmbda[u][i][k]\n","        \n","          ep[i][k] += lmbda[u][i][k] * rp\n","          em[i][k] += lmbda[u][i][k] * rm"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"pCvcLKPHv2Np"},"source":["Ahora podemos calcular los factores:"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{},"colab_type":"code","id":"PSCn7H2Sv38i"},"outputs":[],"source":["a = [[0 for _ in range(NUM_FACTORS)] for _ in range(NUM_USERS)] \n","\n","for u in range(NUM_USERS):\n","  for k in range(NUM_FACTORS):\n","    a[u][k] = gamma[u][k] / sum(gamma[u])"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{},"colab_type":"code","id":"g5rgFop9wdtV"},"outputs":[],"source":["b = [[0 for _ in range(NUM_FACTORS)] for _ in range(NUM_ITEMS)]\n","\n","for i in range(NUM_ITEMS):\n","  for k in range(NUM_FACTORS):\n","    b[i][k] = ep[i][k] / (ep[i][k] + em[i][k])"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"bMCnjpicLA5d"},"source":["## Cálculo de las predicciones\n","\n","Como hemos comentado, calcular la predicción del voto del usuario *u* al item *i* implicar realizar el producto escalar de sus vectores de factores. La siguiente función realiza esta operación:\n"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{},"colab_type":"code","id":"8CtE4DRXLHHX"},"outputs":[],"source":["def compute_prediction (a_u, b_i):\n","  prediction = 0\n","  for k in range(NUM_FACTORS):\n","    prediction += a_u[k] * b_i[k]\n","  return prediction * (MAX_RATING - MIN_RATING) + MIN_RATING"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"RwLhJQRcLT0A"},"source":["## Cálculo de las recomendaciones\n","\n","El cálculo de las recomendaciones, por lo general, simplemente implica seleccionar los *N* items con una predicción más alta. Por ejemplo, si quisiéramos recomendar *N = 3* items a un usuario que tuviera las siguientes predicciones:\n","\n","|   \t| i1 \t| i2 \t| i3 \t| i4 \t| i5 \t| i6 \t| i7 \t| i8 \t| i9 \t| i10 \t|\n","|:-:\t|:--:\t|:--:\t|:--:\t|:--:\t|:--:\t|:--:\t|:--:\t|:--:\t|:--:\t|-----\t|\n","| u \t|   \t|  2,9 \t|    \t|  4,7 \t|  5,0 \t|    \t|  1,2 \t|    \t|   \t|  3,1 \t|\n","\n","Se le recomendarían a dicho usuario los items *i5*, *i4* e *i10*.\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"d25K5LZ8UqZE"},"source":["##Cálculo del MAE\n","\n","En esta sección vamos a mostrar cómo calcular el error medio absoluto (MAE) de las predicciones realizadas por el algoritmo BNMF.\n","\n","Para ello, lo primero que debemos hacer es calcular las predicciones para todos los items que haya recibido una votación de test:"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{},"colab_type":"code","id":"EOx7B3rwUu0i"},"outputs":[],"source":["predictions = [[None for _ in range(NUM_ITEMS)] for _ in range(NUM_USERS)] \n","\n","for u in range(NUM_USERS):\n","  for i in range(NUM_ITEMS):\n","    if test_ratings[u][i] != None:\n","      predictions[u][i] = compute_prediction(a[u], b[i])"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"3DROA4nXUz2M"},"source":["Y, a continuación, calculamos el MAE:"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{},"colab_type":"code","id":"VaSDJhw1U1sM"},"outputs":[],"source":["def get_user_mae (u):\n","  mae = 0\n","  count = 0\n","  \n","  for i in range(NUM_ITEMS):\n","    if test_ratings[u][i] != None and predictions[u][i] != None:\n","      mae += abs(test_ratings[u][i] - predictions[u][i])\n","      count += 1\n","  \n","  if count > 0:\n","    return mae / count\n","  else:\n","    return None"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{},"colab_type":"code","id":"GYi711CXU4TB"},"outputs":[],"source":["def get_mae ():\n","  mae = 0\n","  count = 0\n","  \n","  for u in range(NUM_USERS):\n","    user_mae = get_user_mae(u)\n","      \n","    if user_mae != None:\n","      mae += user_mae\n","      count += 1\n","  \n","  \n","  if count > 0:\n","    return mae / count\n","  else:\n","    return None   "]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"colab_type":"code","executionInfo":{"elapsed":35811,"status":"ok","timestamp":1558733447675,"user":{"displayName":"Fernando Ortega Requena","photoUrl":"https://lh5.googleusercontent.com/-t9XtZoyOrPU/AAAAAAAAAAI/AAAAAAAAA2I/muQCKCLOQqk/s64/photo.jpg","userId":"02003917424124170753"},"user_tz":-120},"id":"EcdujGunU569","outputId":"f571ba75-b7ac-44ae-fe88-d2e2776f192d"},"outputs":[{"name":"stdout","output_type":"stream","text":["System MAE = 0.8409928741700939\n"]}],"source":["mae = get_mae()\n","print(\"System MAE = \" + str(mae))"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"OAZuchyMVFgu"},"source":["## Entendiendo los factores\n","\n","En el modelo BNMF los factores se almacenan en las matrices $a$ y $b$. Como se ha comentado anteriormente, estas matrices tienen una interpretación probabilística. De este modo:\n","\n","- $a_u$ representa una distribución de probabilidad que representa la probabilidad del usuario $u$ de pertenecer a cada cluster.\n","\n","- $b_i$ representa la probabilidad de que a un usuario del cluster $k$ le interese el item $i$."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"y9osKwwRViU7"},"source":["## Referencias\n","\n","Hernando, A., Bobadilla, J., & Ortega, F. (2016). **A non negative matrix factorization for collaborative filtering recommender systems based on a Bayesian probabilistic model**. Knowledge-Based Systems, 97, 188-202."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"N3F5dIirf1EQ"},"source":["---\n","\n","*Este documento ha sido desarrollado por **Fernando Ortega**. Dpto. Sistemas Informáticos, ETSI de Sistemas Informáticos, Universidad Politécnica de Madrid.*\n","\n","*Última actualización: Marzo de 2024*\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"zBL-nJAc_Cgs"},"source":["<p xmlns:cc=\"http://creativecommons.org/ns#\" >This work is licensed under <a href=\"http://creativecommons.org/licenses/by-nc/4.0/?ref=chooser-v1\" target=\"_blank\" rel=\"license noopener noreferrer\" style=\"display:inline-block;\">CC BY-NC 4.0<img style=\"height:22px!important;margin-left:3px;vertical-align:text-bottom;\" src=\"https://mirrors.creativecommons.org/presskit/icons/cc.svg?ref=chooser-v1\"><img style=\"height:22px!important;margin-left:3px;vertical-align:text-bottom;\" src=\"https://mirrors.creativecommons.org/presskit/icons/by.svg?ref=chooser-v1\"><img style=\"height:22px!important;margin-left:3px;vertical-align:text-bottom;\" src=\"https://mirrors.creativecommons.org/presskit/icons/nc.svg?ref=chooser-v1\"></a></p>"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"3.4. BNMF.ipynb","provenance":[],"version":"0.3.2"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.7"}},"nbformat":4,"nbformat_minor":0}
