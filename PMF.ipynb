{"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"VjPUbFbgwPhR"},"source":["# Filtrado Colaborativo: *Probabilistic Matrix Factorization*\n","\n","Durante los inicios del filtrado colaborativo el algoritmo de KNN era el más empleado debido a los buenos resultados que reportaba y a la facilidad con la que podían explicarse sus recomendaciones. Sin embargo, este algoritmo tiene una gran desventaja: su escalabilidad. El algoritmo de KNN funciona bien para datasets de tamaño medio, pero, a medida que el dataset crece, los tiempos de cómputo para obtener las recomendaciones se vuelven inasumibles. Aumentar el número de usuarios y/o el número de items implicar ralentizar el cálculo de las similaridades, la búsqueda de los k vecinos y el número de predicciones a realizar.\n","\n","Como consecuencia de estos problema y del gran empuje que supuso el [Netflix Prize](https://www.netflixprize.com/) (concurso que ofrecía una recompensa de 1M de dólares al equipo que consiguiera mejorar el RMSE en el dataset de Netflix) comenzaron a ganar fuerza los sistemas de filtrado colaborativo basados en modelos, más concretamente los basados en modelos de factorización matricial.\n","\n","El **filtrado colaborativo basado en factorización matricial** se basa en la siguiente idea: las votaciones que los usuarios realizan a los items están condicionadas por una serie de factores latentes intrínsecos a los usuarios y los items. Ilustremos esto con un ejemplo. Supongamos un sistema de recomendación de películas. Lo que postula la factorización matricial es que los usuarios votan las películas basándose no sólo en la propia película, sino que lo hacen basándose en las características que describen esa película. Si a un usuario le gustan las películas de acción con un toque de comedia, es muy probable que le gusten todas las películas de acción con un toque de comedia. Los algoritmos de filtrado colaborativo buscan estas propiedades intrínsecas al dominio en el que se realizan las recomendaciones y las denominan **factores latentes** u ocultos. Es importante resaltar que estos factores son ocultos, y aunque en el ejemplo de la recomendación de películas podamos suponer que se trata de géneros de cine, el modelo nunca nos va a indicar con qué género se corresponde cada factor.\n","\n","Matemáticamente, la factorización matricial consiste en encontrar las matrices $P$ y $Q$ que satisfagan la siguiente expresión:\n","\n","$$R \\approx P \\cdot Q$$\n","\n","En esta expresión:\n","\n","- $R$ representa la matriz (dispersa) con las votaciones de los usuarios (filas) a los items (columnas).\n","- $P$ representa las matriz (densa) de factores de los usuarios (filas) con los *k* factores latentes (columnas).\n","- $Q$ representa las matriz (densa) de factores de los items (columnas) con los *k* factores latentes (filas).\n","\n","Como vemos, los modelos tienen un parámetro que será necesario tunear con el fin de ajustar el modelo a cada dataset. Este parámetro ***k*** representa el número de factores latentes de nuestro modelo.\n","\n","Desarrollando la expresión anterior, podemos inferir que predicción de voto de un usuario $u$ a un item $i$ queda como:\n","\n","$$\\hat{r}_{u,i} = \\vec{p}_u \\cdot \\vec{q}_i$$\n","\n","Dónde $\\vec{p}_u$ representa un vector fila de la matriz $P$ con los factores latentes del usuario $u$ y $\\vec{q}_i$ representa un vector columna de la matriz $Q$ con los factores latentes del item $i$.\n","\n","Por lo tanto, podemos plantear la búsqueda de los factores latentes como un problema de optimización, en el cual buscamos minimizar el error cometido en los votos conocidos:\n","\n","$$\\min_{p,q} \\sum_{(u,i) \\in R} ( r_{u,i} - \\vec{p}_u \\cdot \\vec{q}_i)^2$$\n","\n","Expresión a la que podemos añadir una regularización para evitar el *overfitting*:\n","\n","$$\\min_{p,q} \\sum_{(u,i) \\in R} ( r_{u,i} - \\vec{p}_u \\cdot \\vec{q}_i)^2 + \\lambda (||\\vec{p}_u||^2 + ||\\vec{q}_i||^2)$$\n","\n","Es posible resolver este problema mediante la técnica de descenso de gradiente, para lo cual debemos encontrar la derivada de la expresión anterior respecto del $\\vec{p}_u$ y $\\vec{q}_i$. Al hacerlo obtenemos las siguientes ecuaciones de actualización:\n","\n","$$e_{u,i} = r_{u,i} - \\vec{p}_u \\cdot \\vec{q}_i$$\n","\n","$$\\vec{p}_u = \\vec{p}_u + \\gamma (e_{u,i} \\cdot \\vec{q}_i - \\lambda \\vec{p}_u)$$\n","\n","$$\\vec{q}_i = \\vec{q}_i + \\gamma (e_{u,i} \\cdot \\vec{p}_u - \\lambda \\vec{q}_i)$$\n","\n","Donde $\\lambda$ y $\\gamma$ son dos hyper-parámetros del modelo que queremos aprender.\n","\n","Una vez entrenado el modelo, las matrices $P$ y $Q$ son aprendidas y no necesitan modificarse hasta que la matriz de votaciones cambie sustancialmente. Obtener una predicción una vez el modelo ha aprendido implica, simplemente, realizar el producto escalar de dos vectores de dimensión *k*, que, por lo general, suele ser un valor pequeño.\n","\n","A este algoritmo se le conoce como ***Probabilistic Matriz Factorization (PMF)***.\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"qj19DL6RCIZq"},"source":["## Carga del dataset\n","\n","Para ilustar mejor el funcionamiento el algoritmo PMF, vamos a desarrollar una implementación del mismo.\n","\n","Para ello usaremos el dataset de [MovieLens 100K](https://grouplens.org/datasets/movielens/) que contiene 100.000 votos de 943 usuarios sobre 1682 películas. Este dataset ha sido dividido en votaciones de entrenamiento (80%) y votaciones de test (20%). Además, los códigos de usuarios e items, han sido modificados para que comience en 0 y terminen en el número de (usuarios / items) - 1."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"3Y1rRJ8hCi2A"},"source":["Inicialmente definimos algunas constantes que nos serán necesarias durante la codificación del algoritmo:"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{},"colab_type":"code","id":"j-n03OB1CxVe"},"outputs":[],"source":["import random"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{},"colab_type":"code","id":"f6WGa59gwPhS"},"outputs":[],"source":["NUM_USERS = 943\n","NUM_ITEMS = 1682\n","\n","MIN_RATING = 1\n","MAX_RATING = 5"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"SYYRgNNZtGpE"},"source":["Y cargamos la matriz con las votaciones de entrenamiento:"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{},"colab_type":"code","id":"aGtDGaQEwPhW"},"outputs":[],"source":["ratings = [[None for _ in range(NUM_ITEMS)] for _ in range(NUM_USERS)] \n","\n","with open(\"./ml-100k/u1.base\", \"rb\") as training_file:\n","  for line in training_file:\n","    [u, i, rating] = line.decode(\"utf-8\").split(\"\\t\")[:3]\n","    ratings[int(u)-1][int(i)-1] = int(rating)"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ynbsIDxnC2OI"},"source":["Del mismo modo, cargamos la matriz de votaciones de test:"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{},"colab_type":"code","id":"qJ4y0dY8C5GB"},"outputs":[],"source":["test_ratings = [[None for _ in range(NUM_ITEMS)] for _ in range(NUM_USERS)] \n","\n","with open(\"./ml-100k/u1.test\", \"rb\") as test_file:\n","  for line in test_file:\n","    [u, i, rating] = line.decode(\"utf-8\").split(\"\\t\")[:3]\n","    test_ratings[int(u)-1][int(i)-1] = int(rating)"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"pJ_HI4_pwPhZ"},"source":["## Inicialización del modelo\n","\n","Definimos los parámetros necesarios para implementar la factorización matricial mediante PMF."]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{},"colab_type":"code","id":"OLkqFAFgwPha"},"outputs":[],"source":["NUM_FACTORS = 7\n","LEARNING_RATE = 0.001 # gamma\n","REGULARIZATION = 0.1 # lambda"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"c6LFYsvgwPhd"},"source":["Inicializamos las matrices de factores con valores uniformes aleatorios en el intervalo \\[0, 1]."]},{"cell_type":"code","execution_count":29,"metadata":{"colab":{},"colab_type":"code","id":"GiMfSEhZwPhe"},"outputs":[],"source":["import numpy as np\n","\n","p = np.array([[random.random() for _ in range(NUM_FACTORS)] for _ in range(NUM_USERS)]) \n","q = np.array([[random.random() for _ in range(NUM_FACTORS)] for _ in range(NUM_ITEMS)])"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"YCTcKsnewPhg"},"source":["## Cálculo de las predicciones\n","\n","Como hemos comentado, calcular la predicción del voto del usuario *u* al item *i* implicar realizar el producto escalar de sus vectores de factores. La siguiente función realiza esta operación:\n"]},{"cell_type":"code","execution_count":28,"metadata":{"colab":{},"colab_type":"code","id":"vSJqb_UqwPhh"},"outputs":[],"source":["def compute_prediction (p_u, q_i):\n","  prediction = 0\n","  for k in range(NUM_FACTORS):\n","    prediction += p_u[k] * q_i[k]\n","  return prediction"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"AQAGR9VWHCVe"},"source":["## Aprendizaje de los factores latentes\n","\n","El proceso de entrenamiento implicar aplicar las operaciones de actualización de las matrices de factores hasta que el algoritmo converja. En general, esta convergencia suele prefijarse como el número de iteraciones que realizamos sobre las operaciones de actualización:"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{},"colab_type":"code","id":"XSqfmucPHY0g"},"outputs":[],"source":["NUM_ITERATIONS = 10"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"bCNTid22HbDN"},"source":["Es importante resaltar que sólo debemos actualizar las matrices $P$ y $Q$ empleando los votos existentes en la matriz $R$.\n","\n","El siguiente código ejemplifica el proceso de entrenamiento del algoritmo:"]},{"cell_type":"code","execution_count":30,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":191},"colab_type":"code","executionInfo":{"elapsed":11738,"status":"ok","timestamp":1590593552971,"user":{"displayName":"Fernando Ortega Requena","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgiZKcEyk_b-6E_dNg7_x20idZVEj0N7w-N6pwgBQ=s64","userId":"02003917424124170753"},"user_tz":-120},"id":"1LfIi2j9wPhm","outputId":"cf2a2196-a865-4d4a-8df9-a4918e65ef10"},"outputs":[{"name":"stdout","output_type":"stream","text":["Iteración 1 de 10\n","Iteración 2 de 10\n","Iteración 3 de 10\n","Iteración 4 de 10\n","Iteración 5 de 10\n","Iteración 6 de 10\n","Iteración 7 de 10\n","Iteración 8 de 10\n","Iteración 9 de 10\n","Iteración 10 de 10\n"]}],"source":["for it in range(NUM_ITERATIONS):\n","  print(\"Iteración \" + str(it + 1) + \" de \" + str(NUM_ITERATIONS))\n","    \n","  updated_p = np.array(list(p)) # clone p matrix\n","  updated_q = np.array(list(q)) # clone q matrix\n","    \n","  for u in range(NUM_USERS):\n","    for i in range(NUM_ITEMS):\n","      if ratings[u][i] != None:\n","                \n","        prediction = compute_prediction(p[u], q[i])\n","        rating = ratings[u][i]\n","        error = rating - prediction\n","        \n","        #update p and q using vectorized form\n","        updated_p[u,:] += LEARNING_RATE * (error * q[i,:] - REGULARIZATION * p[u,:])\n","        updated_q[i,:] += LEARNING_RATE * (error * p[u,:] - REGULARIZATION * q[i,:])\n","                \n","        #for k in range(NUM_FACTORS):\n","        #  updated_p[u][k] += LEARNING_RATE * (error * q[i][k] - REGULARIZATION * p[u][k])\n","        #  updated_q[i][k] += LEARNING_RATE * (error * p[u][k] - REGULARIZATION * q[i][k])\n","        \n","  p = updated_p\n","  q = updated_q"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"3nDssq0bIPJJ"},"source":["## Cálculo de las recomendaciones\n","\n","El cálculo de las recomendaciones, por lo general, simplemente implica seleccionar los *N* items con una predicción más alta. Por ejemplo, si quisiéramos recomendar *N = 3* items a un usuario que tuviera las siguientes predicciones:\n","\n","|   \t| i1 \t| i2 \t| i3 \t| i4 \t| i5 \t| i6 \t| i7 \t| i8 \t| i9 \t| i10 \t|\n","|:-:\t|:--:\t|:--:\t|:--:\t|:--:\t|:--:\t|:--:\t|:--:\t|:--:\t|:--:\t|-----\t|\n","| u \t|   \t|  2,9 \t|    \t|  4,7 \t|  5,0 \t|    \t|  1,2 \t|    \t|   \t|  3,1 \t|\n","\n","Se le recomendarían a dicho usuario los items *i5*, *i4* e *i10*."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Wt2nzde9IzES"},"source":["##Cálculo del MAE\n","\n","En esta sección vamos a mostrar cómo calcular el error medio absoluto (MAE) de las predicciones realizadas por el algoritmo PMF\n","\n","Para ello, lo primero que debemos hacer es calcular las predicciones para todos los items que haya recibido una votación de test:"]},{"cell_type":"code","execution_count":31,"metadata":{"colab":{},"colab_type":"code","id":"nU5wPGD_JD_L"},"outputs":[],"source":["predictions = [[None for _ in range(NUM_ITEMS)] for _ in range(NUM_USERS)] \n","\n","# Rellenamos la matriz de predicciones\n","for u in range(NUM_USERS):\n","  for i in range(NUM_ITEMS):\n","    if test_ratings[u][i] != None:\n","      predictions[u][i] = compute_prediction(p[u], q[i])"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"lrRRzuNsJRIw"},"source":["Y, a continuación, calculamos el MAE:"]},{"cell_type":"code","execution_count":32,"metadata":{"colab":{},"colab_type":"code","id":"tvwq1xb_I8v8"},"outputs":[],"source":["def get_user_mae (u):\n","  mae = 0\n","  count = 0\n","  \n","  for i in range(NUM_ITEMS):\n","    if test_ratings[u][i] != None and predictions[u][i] != None:\n","      mae += abs(test_ratings[u][i] - predictions[u][i])\n","      count += 1\n","  \n","  if count > 0:\n","    return mae / count\n","  else:\n","    return None"]},{"cell_type":"code","execution_count":33,"metadata":{"colab":{},"colab_type":"code","id":"ybeiDyy4I-F4"},"outputs":[],"source":["def get_mae ():\n","  mae = 0\n","  count = 0\n","  \n","  for u in range(NUM_USERS):\n","    user_mae = get_user_mae(u)\n","      \n","    if user_mae != None:\n","      mae += user_mae\n","      count += 1\n","  \n","  \n","  if count > 0:\n","    return mae / count\n","  else:\n","    return None   "]},{"cell_type":"code","execution_count":34,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"colab_type":"code","executionInfo":{"elapsed":12101,"status":"ok","timestamp":1590593553526,"user":{"displayName":"Fernando Ortega Requena","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgiZKcEyk_b-6E_dNg7_x20idZVEj0N7w-N6pwgBQ=s64","userId":"02003917424124170753"},"user_tz":-120},"id":"yhJ3ez3cJBBa","outputId":"9e4cb768-aea6-406b-d2ba-a971f3e5d270"},"outputs":[{"name":"stdout","output_type":"stream","text":["System MAE = 0.9003487046268857\n"]}],"source":["mae = get_mae()\n","print(\"System MAE = \" + str(mae))"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"NCLMvQKmKrQ5"},"source":["## Añadiendo los bias\n","\n","El modelo descrito anteriormente mejora significativamente la escalabilidad del filtrado colaborativo y, además, incremente notablemente la calidad de las predicciones y recomendaciones. Sin embargo, dicho modelo no se ajusta a la realidad puesto que no refleja los sesgos que los usuarios tienen cuando realizan votaciones.\n","\n","Parece evidente pensar que no todos los usuarios tienen la misma interpretación de las votaciones. Por ejemplo, existen usuarios más \"generosos\" con las votaciones que tienden a asignar siempre valoraciones altas y existen usuarios más \"tacaños\" con las votaciones que tienden a asignar siempre valoraciones más bajas. Que el primer usuario valore un item con 5 y el segundo usuario valore el mismo item con un 4 no quiere decir que al primero le haya gustado más el item. Cada usuario hace su propia interpretación de lo que significan los votos 4 y 5.\n","\n","Igualmente, existen determinados items que socialmente tienen que gustar y existen otros items que está \"mal visto\" que gusten. Por ejemplo, resulta extraño que alguien pueda otorgar la nota mínima a *El Padrino* aunque no le haya gustado. La presión social hace que dicha película sea importante, y eso condiciona nuestro voto sobre la misma. Igualmente, resulta extraño que alguien pueda otorgar la nota máxima a *Sharknado* ya que, socialmente, es considerada una película \"mala\".\n","\n","Para reflejar este fenómeno dentro de nuestro modelo de factorización matricial, debemos hacer algunas modificaciones sobre el mismo. Para empezar, cambiaremos cómo se calculan las predicciones:\n","\n","$$\\hat{r}_{u,i} = \\mu + b_u + b_i + \\vec{p}_u \\cdot \\vec{q}_i$$\n","\n","Donde $\\mu$ representa la votación media de la base de datos, $b_u$ representa el bias (sesgo) del usuario $u$, $b_i$ representa el bias (sesgo) del item $i$ y $\\vec{p}_u \\cdot \\vec{q}_i$ simboliza la interacción entre el usuario $u$ y el item $i$. \n","\n","De este modo, la predicción será calculada como la media de la base de datos, +/- un ajuste en función de cómo suele vota el usuario, +/- un ajuste de cómo suele votarse el item, y +/- la interacción entre el usuario y el item.\n","\n","Debido a este cambio, la función a minimizar es ahora la siguiente:\n","\n","$$\\min_{b_u, b_i,p,q} \\sum_{(u,i) \\in R} ( r_{u,i} - \\mu - b_u - b_i - \\vec{p}_u \\cdot \\vec{q}_i)^2 + \\lambda (||\\vec{p}_u||^2 + ||\\vec{q}_i||^2 + b_u^2 + b_i^2)$$\n","\n","A la que, tras aplicar la derivada respecto de $b_u$, $q_i$, $\\vec{p}_u$ y $\\vec{q}_i$ obtenemos:\n","\n","$$e_{u,i} = r_{u,i} - \\mu - b_u - b_i - \\vec{p}_u \\cdot \\vec{q}_i$$\n","\n","$$b_u = b_u + \\gamma (e_{u,i} - \\lambda b_u)$$\n","\n","$$b_i = b_i + \\gamma (e_{u,i} - \\lambda b_i)$$\n","\n","$$\\vec{p}_u = \\vec{p}_u + \\gamma (e_{u,i} \\cdot \\vec{q}_i - \\lambda \\vec{p}_u)$$\n","\n","$$\\vec{q}_i = \\vec{q}_i + \\gamma (e_{u,i} \\cdot \\vec{p}_u - \\lambda \\vec{q}_i)$$"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"EjjXM5XMPqHv"},"source":["Definimos una nueva función para calcular las predicciones:"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{},"colab_type":"code","id":"jRgqj7cSP4P8"},"outputs":[],"source":["def compute_biased_prediction (avg, b_u, b_i, p_u, q_i):\n","  deviation = 0\n","  for k in range(NUM_FACTORS):\n","    deviation += p_u[k] * q_i[k]\n","        \n","  prediction = avg + b_u + b_i + deviation\n","  return prediction"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"B1NUpz4-P-Yd"},"source":["Calculamos el voto medio:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{},"colab_type":"code","id":"-tyed6WXQAC3"},"outputs":[],"source":["rating_average = 0\n","rating_count = 0\n","\n","for u in range(NUM_USERS):\n","  for i in range(NUM_ITEMS):\n","    if ratings[u][i] != None:\n","      rating_average += ratings[u][i]\n","      rating_count += 1\n","      \n","rating_average /= rating_count    "]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"1a9_dMy_QVQ2"},"source":["Reiniciamos las matrices de factores y los vectores de bias con valores aleatorios en el intervalo \\[0, 1]:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{},"colab_type":"code","id":"qMnjONUwQbH0"},"outputs":[],"source":["p = [[random.random() for _ in range(NUM_FACTORS)] for _ in range(NUM_USERS)] \n","q = [[random.random() for _ in range(NUM_FACTORS)] for _ in range(NUM_ITEMS)] \n","\n","bu = [random.random() for _ in range(NUM_USERS)]\n","bi = [random.random() for _ in range(NUM_ITEMS)]"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"RGF76kqiStDb"},"source":["Y volvemos a entrenar nuestro modelo:"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":191},"colab_type":"code","executionInfo":{"elapsed":21330,"status":"ok","timestamp":1590593562815,"user":{"displayName":"Fernando Ortega Requena","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgiZKcEyk_b-6E_dNg7_x20idZVEj0N7w-N6pwgBQ=s64","userId":"02003917424124170753"},"user_tz":-120},"id":"4kQQMPBoQdDG","outputId":"48f4c451-55b8-4927-ee5f-d7a1b76c1507"},"outputs":[{"name":"stdout","output_type":"stream","text":["Iteración 1 de 10\n","Iteración 2 de 10\n","Iteración 3 de 10\n","Iteración 4 de 10\n","Iteración 5 de 10\n","Iteración 6 de 10\n","Iteración 7 de 10\n","Iteración 8 de 10\n","Iteración 9 de 10\n","Iteración 10 de 10\n"]}],"source":["for it in range(NUM_ITERATIONS):\n","  print(\"Iteración \" + str(it + 1) + \" de \" + str(NUM_ITERATIONS))\n","    \n","  updated_p = list(p) # clone p matrix\n","  updated_q = list(q) # clone q matrix\n","    \n","  updated_bu = list(bu) # clone bu vector\n","  updated_bi = list(bi) # clone bi vector\n","    \n","  for u in range(NUM_USERS):\n","    for i in range(NUM_ITEMS):\n","      if ratings[u][i] != None:\n","                \n","        prediction = compute_biased_prediction(rating_average, bu[u], bi[i], p[u], q[i])                \n","        rating = ratings[u][i]\n","        error = rating - prediction\n","                \n","        for k in range(NUM_FACTORS):\n","          updated_p[u][k] += LEARNING_RATE * (error * q[i][k] - REGULARIZATION * p[u][k])\n","          updated_q[i][k] += LEARNING_RATE * (error * p[u][k] - REGULARIZATION * q[i][k])\n","                    \n","        updated_bu[u] += LEARNING_RATE * (error - REGULARIZATION * bu[u])\n","        updated_bi[i] += LEARNING_RATE * (error - REGULARIZATION * bi[i])\n","\n","        \n","  p = updated_p\n","  q = updated_q\n","    \n","  bu = updated_bu\n","  bi = updated_bi"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"xltQD7GiViev"},"source":["Calculamos las nuevas predicciones:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{},"colab_type":"code","id":"S_WodJyQVkXV"},"outputs":[],"source":["predictions = [[None for _ in range(NUM_ITEMS)] for _ in range(NUM_USERS)] \n","\n","for u in range(NUM_USERS):\n","  for i in range(NUM_ITEMS):\n","    if test_ratings[u][i] != None:\n","      predictions[u][i] = compute_biased_prediction(rating_average, bu[u], bi[i], p[u], q[i])"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ClKgy0pIRcec"},"source":["Y calculamos el nuevo MAE:"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"colab_type":"code","executionInfo":{"elapsed":21241,"status":"ok","timestamp":1590593562823,"user":{"displayName":"Fernando Ortega Requena","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgiZKcEyk_b-6E_dNg7_x20idZVEj0N7w-N6pwgBQ=s64","userId":"02003917424124170753"},"user_tz":-120},"id":"bXcMof_JReRc","outputId":"f51de5ed-990a-49f7-bd97-9bac1b817fdb"},"outputs":[{"name":"stdout","output_type":"stream","text":["System MAE = 0.9597400939551685\n"]}],"source":["mae = get_mae()\n","print(\"System MAE = \" + str(mae))"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"-yWizA0nWvg1"},"source":["## Referencias\n","\n","Mnih, A., & Salakhutdinov, R. R. (2008). **Probabilistic matrix factorization**. In Advances in neural information processing systems (pp. 1257-1264).\n","\n","Koren, Y., Bell, R., & Volinsky, C. (2009). **Matrix factorization techniques for recommender systems**. Computer, (8), 30-37."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"f6PVZh5Jfzm7"},"source":["---\n","\n","*Este documento ha sido desarrollado por **Fernando Ortega**. Dpto. Sistemas Informáticos, ETSI de Sistemas Informáticos, Universidad Politécnica de Madrid.*\n","\n","*Última actualización: Marzo de 2024*\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"6jddkoZe--hb"},"source":["<p xmlns:cc=\"http://creativecommons.org/ns#\" >This work is licensed under <a href=\"http://creativecommons.org/licenses/by-nc/4.0/?ref=chooser-v1\" target=\"_blank\" rel=\"license noopener noreferrer\" style=\"display:inline-block;\">CC BY-NC 4.0<img style=\"height:22px!important;margin-left:3px;vertical-align:text-bottom;\" src=\"https://mirrors.creativecommons.org/presskit/icons/cc.svg?ref=chooser-v1\"><img style=\"height:22px!important;margin-left:3px;vertical-align:text-bottom;\" src=\"https://mirrors.creativecommons.org/presskit/icons/by.svg?ref=chooser-v1\"><img style=\"height:22px!important;margin-left:3px;vertical-align:text-bottom;\" src=\"https://mirrors.creativecommons.org/presskit/icons/nc.svg?ref=chooser-v1\"></a></p>"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"3.2. PMF.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.7"}},"nbformat":4,"nbformat_minor":0}
